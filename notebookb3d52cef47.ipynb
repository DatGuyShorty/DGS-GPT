{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7731388,"sourceType":"datasetVersion","datasetId":4517792},{"sourceId":13197,"sourceType":"modelInstanceVersion","modelInstanceId":10914,"modelId":15305}],"dockerImageVersionId":30839,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install tiktoken","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport pickle\nimport mmap\nimport random\nimport os\nimport logging \nimport tiktoken\nimport re\nimport unicodedata\nimport time\nimport math","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Conservative settings for 15GB VRAM\nbatch_size = 2\ncontext_win = 512\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nn_embd = 1024  # Must be even for RoPE\nn_layer = 4\nn_head = 4   # Must be divisible by n_query_groups (2)\nmax_epochs = 5\ndropout = 0.1\nvocab_size = 100277\ntorch.manual_seed(2021)\n\n# Verify dimensions\nassert n_embd % 2 == 0, \"n_embd must be even for RoPE\"\nassert n_embd % n_head == 0, \"n_embd must be divisible by n_head\"\nassert n_head % 2 == 0, \"n_head must be divisible by n_query_groups\"\n\n# Verify dimensions\nassert n_embd % 2 == 0, \"n_embd must be even for RoPE\"\nassert n_embd % n_head == 0, \"n_embd must be divisible by n_head\"\n\n# Estimated VRAM usage: ~13GB\n# Will need gradient checkpointing enabled","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Standard settings for 40GB VRAM\n# batch_size = 16        # Your original setting\n# context_win = 8192     # Your original setting\n# learning_rate = 5e-4\n# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n# n_embd = 4096         # Your original setting\n# n_layer = 32          # Your original setting\n# n_head = 32           # Your original setting\n# max_epochs = 5\n# dropout = 0.2\n# vocab_size = 100277\n# torch.manual_seed(2021)\n\n# # Estimated VRAM usage: ~32GB\n# # Can run without gradient checkpointing","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Expanded settings for 100GB+ VRAM\n# batch_size = 32        # Larger batch size\n# context_win = 16384    # Doubled context window\n# learning_rate = 8e-4   # Slightly higher learning rate\n# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n# n_embd = 5120         # Larger embedding size\n# n_layer = 40          # More layers\n# n_head = 40           # More attention heads\n# max_epochs = 5\n# dropout = 0.2\n# vocab_size = 100277\n# torch.manual_seed(2021)\n\n# # Estimated VRAM usage: ~80GB\n# # Can enable additional features like:\n# # - Larger expert count in MoE\n# # - Bigger batch sizes\n# # - Model parallelism","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_query_groups = 2  # for GQA\nwindow_size = 256   # for sliding window attention\nnum_experts = 8     # for sparse MoE\nnum_active = 2      # for sparse MoE\ncapacity_factor = 1.2  # for sparse MoE","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"warmup_steps = 1000\nmax_lr = learning_rate\nmin_lr = learning_rate * 0.1","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #only for TPU run\n# num_cores = xm.xrt_world_size()\n# # Select the TPU device\n# device = xm.xla_device()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"enc = tiktoken.get_encoding(\"cl100k_base\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eval_interval = 1000\neval_iters = 200","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_file = r\"/kaggle/input/small-web-crawled-text/webcrawled_train.txt\"\nval_file = r\"/kaggle/input/small-web-crawled-text/webcrawled_val.txt\"","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"logging.basicConfig(filename='data_loading.log', level=logging.WARNING)  \n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_batch(split):\n    global current_position \n    filename = train_file if split == 'train' else val_file\n    filesize = os.path.getsize(filename)\n    \n    try:\n        with open(filename, 'r', encoding='utf-8', errors='replace') as f:\n            # Reset position if we're near the end of file\n            if current_position >= filesize - (context_win * batch_size):\n                current_position = 0\n            \n            f.seek(current_position)\n            \n            # Read a larger chunk to ensure we have enough data\n            required_size = context_win * batch_size * 2  # Double size for safety\n            data_chunk = f.read(required_size)\n            \n            if not data_chunk:\n                current_position = 0\n                return None, None\n                \n            # Encode the chunk\n            tokens = enc.encode(data_chunk)\n            data = torch.tensor(tokens, dtype=torch.long)\n            \n            # Ensure we have enough data for the context window\n            if len(data) <= context_win:\n                current_position = 0\n                return None, None\n                \n            # Calculate valid random indices\n            max_idx = len(data) - context_win - 1\n            if max_idx < 1:\n                current_position = 0\n                return None, None\n                \n            # Generate random indices and create batches\n            ix = torch.randint(0, max_idx, (batch_size,))\n            x = torch.stack([data[i:i+context_win] for i in ix])\n            y = torch.stack([data[i+1:i+context_win+1] for i in ix])\n            \n            # Update position\n            current_position += len(data_chunk) // 2  # Move forward by half the chunk\n            \n            # Move to device\n            x, y = x.to(device), y.to(device)\n            \n            return x, y\n            \n    except Exception as e:\n        print(f\"Error in get_batch: {str(e)}\")\n        print(f\"Current position: {current_position}\")\n        print(f\"File size: {filesize}\")\n        current_position = 0\n        return None, None","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss():\n        out = {}\n        model.eval()\n        for split in ['train', 'val']:\n            losses = torch.zeros(eval_iters)\n            for k in range(eval_iters):\n                X, Y = get_batch(split)\n                if X is None:  # Handle case when get_batch returns None\n                    continue\n                logits, loss = model(X, Y)\n                losses[k] = loss.item()\n            out[split] = losses.mean()\n        model.train()\n        return out","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CosineWarmupScheduler:\n    def __init__(self, optimizer, warmup_steps, max_lr, min_lr):\n        self.optimizer = optimizer\n        self.warmup_steps = warmup_steps\n        self.max_lr = max_lr\n        self.min_lr = min_lr\n        self.current_step = 0\n        \n    def step(self):\n        self.current_step += 1\n        if self.current_step < self.warmup_steps:\n            # Linear warmup\n            lr = self.max_lr * (self.current_step / self.warmup_steps)\n        else:\n            # Cosine decay\n            progress = (self.current_step - self.warmup_steps) / (num_batches_per_epoch * max_epochs - self.warmup_steps)\n            lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + math.cos(math.pi * progress))\n            \n        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = lr\n        return lr","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Head(nn.Module):\n    def __init__(self,head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd,head_size)\n        self.query = nn.Linear(n_embd,head_size)\n        self.value = nn.Linear(n_embd,head_size)\n        self.register_buffer('tril',torch.tril(torch.ones(context_win,context_win)))\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self,x):\n        B,T,C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5\n        wei = wei.masked_fill(self.tril[:T,:T]==0,float('-inf'))\n        wei = F.softmax(wei,dim=-1)\n        wei = self.dropout(wei)\n        v = self.value(x)\n        out = wei @ v\n        return out\n        ","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GroupedQueryAttention(nn.Module):\n    def __init__(self, n_embd, n_head, n_query_groups=2):\n        super().__init__()\n        assert n_head % n_query_groups == 0, \"n_head must be divisible by n_query_groups\"\n        \n        self.n_head = n_head\n        self.n_query_groups = n_query_groups\n        self.head_size = n_embd // n_head\n        \n        # Adjust dimensions for grouped queries\n        self.key = nn.Linear(n_embd, n_embd)\n        self.value = nn.Linear(n_embd, n_embd)\n        self.query = nn.Linear(n_embd, n_embd * n_query_groups // n_head)\n        \n        self.register_buffer('tril', torch.tril(torch.ones(context_win, context_win)))\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        B, T, C = x.shape  # batch, sequence length, embedding dimension\n        \n        # Split heads while keeping query groups\n        k = self.key(x).view(B, T, self.n_head, self.head_size).transpose(1, 2)\n        v = self.value(x).view(B, T, self.n_head, self.head_size).transpose(1, 2)\n        q = self.query(x).view(B, T, self.n_query_groups, self.head_size).transpose(1, 2)\n        \n        # Repeat KV for each query group\n        heads_per_group = self.n_head // self.n_query_groups\n        k = k.reshape(B, self.n_query_groups, heads_per_group, T, self.head_size)\n        v = v.reshape(B, self.n_query_groups, heads_per_group, T, self.head_size)\n        q = q.unsqueeze(2)  # Add dimension for heads_per_group\n        \n        # Attention scores\n        att = (q @ k.transpose(-2, -1)) * (self.head_size ** -0.5)\n        att = att.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        att = self.dropout(att)\n        \n        # Combine heads\n        out = (att @ v).transpose(2, 3).contiguous()\n        out = out.view(B, T, C)\n        \n        return out","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SlidingWindowAttention(nn.Module):\n    def __init__(self, n_embd, window_size=256):\n        super().__init__()\n        self.window_size = window_size\n        self.query = nn.Linear(n_embd, n_embd)\n        self.key = nn.Linear(n_embd, n_embd)\n        self.value = nn.Linear(n_embd, n_embd)\n        \n    def forward(self, x):\n        B, T, C = x.shape\n        q = self.query(x)\n        k = self.key(x)\n        v = self.value(x)\n        \n        # Create sliding windows\n        padding = self.window_size // 2\n        padded_k = F.pad(k, (0, 0, padding, padding))\n        padded_v = F.pad(v, (0, 0, padding, padding))\n        \n        # Compute attention for each position using only its window\n        output = []\n        for i in range(T):\n            start = i\n            end = i + self.window_size\n            window_k = padded_k[:, start:end]\n            window_v = padded_v[:, start:end]\n            \n            att = (q[:, i:i+1] @ window_k.transpose(-2, -1)) / math.sqrt(C)\n            att = F.softmax(att, dim=-1)\n            out = att @ window_v\n            output.append(out)\n            \n        return torch.cat(output, dim=1)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class RotaryEmbedding(nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048):\n        super().__init__()\n        # Make sure dim is even\n        dim = dim if dim % 2 == 0 else dim - 1\n        \n        # Create inverse frequency bands\n        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer('inv_freq', inv_freq)\n        self.max_position_embeddings = max_position_embeddings\n        self.dim = dim\n\n    def forward(self, x, seq_len):\n        B, T, C = x.shape\n        \n        # Create position indices\n        t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n        \n        # Calculate frequencies\n        freqs = torch.einsum('i,j->ij', t, self.inv_freq)  # [seq_len, dim/2]\n        \n        # Calculate cos and sin\n        cos = freqs.cos()  # [seq_len, dim/2]\n        sin = freqs.sin()  # [seq_len, dim/2]\n        \n        # Expand dimensions for broadcasting\n        cos = cos.view(1, T, -1)  # [1, seq_len, dim/2]\n        sin = sin.view(1, T, -1)  # [1, seq_len, dim/2]\n        \n        # Duplicate for all batch elements\n        cos = cos.expand(B, -1, -1)  # [batch, seq_len, dim/2]\n        sin = sin.expand(B, -1, -1)  # [batch, seq_len, dim/2]\n        \n        # Split input into even and odd dimensions\n        x1 = x[..., ::2]  # [batch, seq_len, dim/2]\n        x2 = x[..., 1::2]  # [batch, seq_len, dim/2]\n        \n        # Apply rotation\n        rotated_x = torch.cat([\n            x1 * cos - x2 * sin,\n            x2 * cos + x1 * sin\n        ], dim=-1)  # [batch, seq_len, dim]\n        \n        return rotated_x","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SparseExpertLayer(nn.Module):\n    def __init__(self, n_embd, num_experts=8, num_active=2, capacity_factor=1.2):\n        super().__init__()\n        self.num_experts = num_experts\n        self.num_active = num_active\n        self.capacity_factor = capacity_factor\n        \n        # Expert capacity\n        self.expert_capacity = int(capacity_factor * batch_size * context_win / num_experts)\n        \n        self.gate = nn.Linear(n_embd, num_experts)\n        self.experts = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(n_embd, 4 * n_embd),\n                nn.GELU(),  # Using GELU instead of ReLU\n                nn.Linear(4 * n_embd, n_embd),\n                nn.Dropout(dropout)\n            ) for _ in range(num_experts)\n        ])\n        \n    def forward(self, x):\n        B, T, C = x.shape\n        x_flat = x.view(-1, C)\n        \n        # Calculate gates and capacities\n        gate_logits = self.gate(x_flat)\n        gates = F.softmax(gate_logits, dim=-1)\n        \n        # Calculate expert assignment\n        expert_weights, expert_indices = torch.topk(gates, self.num_active, dim=-1)\n        expert_weights = expert_weights / expert_weights.sum(dim=-1, keepdim=True)\n        \n        # Dispatch to experts with capacity limits\n        final_output = torch.zeros_like(x_flat)\n        total_tokens = B * T\n        \n        for expert_idx in range(self.num_experts):\n            # Find tokens routed to this expert\n            expert_mask = (expert_indices == expert_idx).any(dim=-1)\n            expert_count = expert_mask.sum().item()\n            \n            if expert_count > 0:\n                if expert_count > self.expert_capacity:\n                    # Randomly drop tokens if over capacity\n                    perm = torch.randperm(expert_count)[:self.expert_capacity]\n                    expert_mask[expert_mask.nonzero()[perm]] = False\n                \n                # Process tokens for this expert\n                expert_input = x_flat[expert_mask]\n                processed = self.experts[expert_idx](expert_input)\n                final_output[expert_mask] = processed\n        \n        return final_output.view(B, T, C)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Block(nn.Module):\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        # Ensure n_head is divisible by n_query_groups\n        n_query_groups = 2\n        assert n_head % n_query_groups == 0, \"n_head must be divisible by n_query_groups\"\n        \n        self.gqa = GroupedQueryAttention(n_embd, n_head, n_query_groups)\n        self.sliding_attention = SlidingWindowAttention(n_embd)\n        self.rope = RotaryEmbedding(n_embd)\n        self.sparse_ffwd = SparseExpertLayer(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        \n        # Apply layer norm and RoPE\n        x_ln = self.ln1(x)\n        x_rope = self.rope(x_ln, T)\n        \n        # Multi-scale attention\n        x = x + self.gqa(x_rope)\n        x = x + self.sliding_attention(self.ln1(x))\n        \n        # Sparse MoE\n        x = x + self.sparse_ffwd(self.ln2(x))\n        return x","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GPTModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n        \n    def forward(self, index, target=None):\n        B, T = index.shape\n        \n        # Token embeddings\n        x = self.token_embedding_table(index)  # [B, T, n_embd]\n        \n        # Process through blocks\n        x = self.blocks(x)  # [B, T, n_embd]\n        x = self.ln_f(x)\n        logits = self.lm_head(x)  # [B, T, vocab_size]\n        \n        if target is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            target = target.view(B*T)\n            loss = F.cross_entropy(logits, target)\n            \n        return logits, loss\n            \n    def generate(self, index, max_size):\n        for _ in range(max_size):\n            index_cropped = index[:, -context_win:]\n            logits, loss = self(index_cropped)\n            logits = logits[:, -1, :]\n            prob = F.softmax(logits, dim=-1)\n            next_word = torch.multinomial(prob, num_samples=1)\n            index = torch.cat((index, next_word), dim=1)\n        return index    \n\n# Create a new model instance with MoE\nmodel = GPTModel()\nmodel = model.to(device)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# checkpoint = torch.load(r'/kaggle/working/D:\\gpt saved model/model_iter_300000.pt', map_location=torch.device('cuda')) \n# model.load_state_dict(checkpoint)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\nscheduler = CosineWarmupScheduler(optimizer, warmup_steps, max_lr, min_lr)\n\ncurrent_lr = scheduler.step()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_batches_per_epoch = 2200000\nnum_batches_per_epoch","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_dir = r'D:\\gpt saved model'\nos.makedirs(save_dir, exist_ok=True)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_model(model, counter):\n    model_path = os.path.join(save_dir, f'model_iter_{counter}.pt')\n    torch.save(model.state_dict(), model_path)\n    print(f\"Model saved at iteration {counter}\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_perplexity(model):\n    total_loss = 0\n    total_tokens = 0\n    num_batches_per_validation = 32  # Define the number of batches for validation\n    \n    # Set the model to evaluation mode\n    model.eval()\n    \n    with torch.no_grad():\n        for _ in range(num_batches_per_validation):\n            try:\n                # Get a batch of validation data using get_batch() function\n                xb, yb = get_batch('val')  # Assuming 'val' is the split for validation data\n                \n                if xb is None:\n                    break  # End of validation data\n                \n                # Forward pass and loss estimation\n                losses = estimate_loss()  # Using estimate_loss() function\n                loss = losses['val']\n                \n                # Update total loss and total tokens\n                total_loss += loss.item() * yb.numel()  # Multiplying loss by number of tokens\n                total_tokens += yb.numel()  # Count the number of tokens in targets (yb)\n            \n            except Exception as e:\n                continue\n    \n    # Calculate perplexity\n    average_loss = total_loss / total_tokens\n    perplexity = torch.exp(torch.tensor(average_loss))\n    \n    return perplexity\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"current_position = 0\ncounter = 0","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for epoch in range(max_epochs):\n    for iter in range(num_batches_per_epoch):\n        # Sample a batch of data (random or sequential, based on your get_batch implementation)\n            start = time.time()\n            xb, yb = get_batch('train')\n            if xb is None:\n                continue\n            else:\n                # Update the counter for every iteration\n                counter += 1\n\n                # Evaluate the loss\n                logits, loss = model(xb, yb)\n                optimizer.zero_grad(set_to_none=True)\n                loss.backward()\n                optimizer.step()\n                if counter % eval_interval == 0 or epoch == max_epochs - 1:\n                                cal = time.time() \n                                endtime = (cal - start)%60\n                                losses = estimate_loss() \n                                print(f\"Iteration {counter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, total time taken: {endtime},current position {current_position}\")\n                                print(enc.decode(model.generate(torch.zeros((1,1),dtype=torch.long,device=device),max_size=256)[0].tolist()))\n                                print('\\n')\n                                print(\"------------\")\n                # Check if it's time to save the model\n                if counter % 100 == 0 :\n                    print(\".\",end=\"\", flush=True)\n                if counter % 5000 == 0:\n                    save_model(model, counter)\n                    \n                # Reset current_position if we've reached the end of the epoch \n                if iter == num_batches_per_epoch - 1: \n                    current_position = 0\n ","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"current_position,counter","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#previous current_position,counter (135236057, 35524)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"perplexity = calculate_perplexity(model)\nprint(f\"Iteration {counter}: Perplexity: {perplexity}\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(enc.decode(model.generate(torch.zeros((1,1),dtype=torch.long,device=device),max_size=1500)[0].tolist()))","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Iteration {counter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_model(model, counter)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null}]}